[{"content":"","permalink":"http://localhost:1313/arts/durgamaa/","summary":"","title":"Durga Maa - à¤œà¤¯ à¤¦à¥à¤°à¥à¤—à¤¾ à¤®à¤¾à¤ğŸ™ğŸŒ¼"},{"content":"","permalink":"http://localhost:1313/arts/lordshiva/","summary":"","title":"Lord Shiva"},{"content":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write theÂ f(x)Â such that when users feed the inputÂ xÂ intoÂ f(x), it gives the correct outputÂ y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x). The goal here is to find the f(x) that transforms the input x into the output y. Well, thatâ€™s not an easy job. In this article, we will learn how this happens.\nDataset To visualize the dataset, letâ€™s make our synthetic dataset where each data point (inputÂ x) is 3 dimensional, making it suitable to be plotted on a 3D chart. We will generate 250 pointsÂ (cluster 0)Â in a cluster centered at the origin (0, 0, 0). A similar cluster of 250 pointsÂ (cluster 1)Â is generated but not centered at the origin. Both clusters are relatively close but there is a clear separation as seen in the image below. These two clusters are the two classes of data points. The big green dot represents the centroid of the whole dataset.\nAfter generating the dataset, we will normalize it by subtracting the mean and dividing by the standard deviation. This is done to zero-center the data and map values in each dimension in the dataset to a common scale. This speeds up the learning.\nThe data will be saved in an array X containing the 3D coordinates of normalized points. We will also generate an array Y with the value either 0 or 1 at each index depending on which cluster the 3D point belongs.\nLearnable Function Now that we have our data ready, we can say that we have theÂ xÂ andÂ y.Â We know that the dataset is linearly separable implying that there is a plane that can divide the dataset into the two clusters, but we donâ€™t know what the equation of such an optimal plane is. For now, letâ€™s just take a random plane.\nThe function f(x) should take a 3D coordinate as input and output a number between 0 and 1. If this number is less than 0.5, this point belongs to cluster 0 otherwise, it belongs to cluster 1. Letâ€™s define a simple function for this task.\nx: input tensor of shape (num_points, 3)W: Weight (parameter) of shape (3, 1) chosen randomlyB: Bias (parameter) of shape (1, 1) chosen randomlySigmoid: A function that maps values between 0 and 1\nLetâ€™s take a moment to understand what this function means.Â Before applying the sigmoid function, we are simply creating a linear mapping from the 3D coordinate (input) to 1D output. Therefore,Â this function will squish the whole 3D space onto a lineÂ meaning that each point in the original 3D space will now be lying somewhere on this line.Â Since this line will extend to infinity,Â we map it toÂ [0, 1]Â using theÂ SigmoidÂ function. As a result, for each given input,Â f(x)Â will output a value between 0 and 1.\nRemember that W and B are chosen randomly and so the 3D space will be squished onto a random line. The decision boundary for this transformation is the set of points that makeÂ f(x)Â = 0.5. Think why! As the 3D space is being squished onto a 1D line, a whole plane is mapped to the value 0.5 on the line. This plane is the decision boundary for f(x). Ideally, it should divide the dataset into two clusters butÂ sinceÂ WÂ andÂ BÂ are randomly chosen, this plane is randomly oriented as shown below.\nOur goal is to find the right values for W and B that orients this plane (decision boundary) in such a way that it divides the dataset into the two clusters. This when done, yields a plane as shown below.\nLoss So, we are now at the starting point (random decision boundary) and we have defined the goal. We need a metric to decide how far we are from the goal. The output of the classifier is a tensor of shape (num_points, 1) where each value is betweenÂ [0, 1]. If you think carefully, these values are just the probabilities of the points belonging to cluster 1. So, we can say that:\nf(x) = P(x belongs to cluster 1) 1-f(x) = P(x belongs to cluster 0) It wouldnâ€™t be wrong to say that [1-f(x), f(x)] forms a probability distribution over the clusters 0 and cluster 1 respectively. This is theÂ predicted probability distribution. We know for sure which cluster every point in the dataset belongs to (fromÂ y). So, we also have theÂ true probability distributionÂ as:\n[0, 1] when x belongs to the cluster 1 [1, 0] when x belongs to the cluster 0 A good metric to calculate the incongruity between two probability distributions is theÂ Cross-EntropyÂ function. As we are dealing with just 2 classes, we can useÂ Binary Cross-Entropy (BCE).Â This function is available in PyTorchâ€™sÂ torch.nnÂ module. If the predicted probability distribution is very similar to the true probability distribution, this function returns a small value and vice versa. We can average this value for all the data points and use it as a parameter to test how the classifier is performing.\nThis value is called the loss and mathematically, our goal now is to minimize this loss.\nTraining Now that we have defined our goal mathematically, how do we reach our goal practically? In other words, how do we find optimal values forÂ WÂ andÂ B? To understand this, we will take a look at some basic calculus. Recall that we currently have random values forÂ WÂ andÂ B.Â The process of learning or training or reaching the goal or minimizing the loss can be divided into two steps:\nForward-propagation: We feed the dataset through the classifier f(x) and use BCE to find the loss. Backpropagation: Using the loss, adjust the values of W and B to minimize the loss. The above two steps will be repeated over and over again until the loss stops decreasing. In this condition, we say that we have reached the goal!\nBackpropagation Forward propagation is simple and already discussed above. However, it is essential to take a moment to understandÂ backpropagationÂ as it is the key to machine learning. Recall that we have 3 parameters (variables) inÂ WÂ and 1 inÂ B. So, in total, we have 4 values to optimize.\nOnce we have the loss from forward-propagation, we will calculate the gradients of the loss function with respect to each variable in the classifier. If we plot the loss for different values of each parameter, we can see that the loss is minimum at a particular value for each parameter. I have plotted the loss vs parameter for each parameter.\nAn important observation to make here is that the loss is minimized at a particular value for each of these parameters as shown by the red dot.\nLetâ€™s consider the first plot and discuss how w1 will be optimized. The process remains the same for the other parameters. Initially, the values for W and B are chosen randomly and soÂ (w1, loss)Â will be randomly placed on this curve as shown by the green dot.\nNow, the goal is to reach the red dot, starting from the green dot. In other words, we need to move downhill. Looking at the slope of the curve at the green dot, we can tell that increasing w1 (moving right) will lower the loss and therefore move the green dot closer to the red one. In mathematical terms, if the gradient of the loss with respect to w1 is negative, increase w1 to move downhill and vice versa. Therefore, w1 should be updated as:\nThe equation above is known asÂ gradient descent equation. Here, theÂ learning_rateÂ controls how much we want to increase or decrease w1. If the learning_rate is large, the update will be large. This could lead to w1 going past the red dot and therefore missing the optimal value. If this value is too small, it will take forever for w1 to reach the red dot. You can try experimenting with different values of learning rate to see which works the best. In general, small values likeÂ 0.01Â works well for most cases.\nIn most cases, a single update is not enough to optimize these parameters; so, the process of forward-propagation and backpropagation is repeated in a loop until the loss stops reducing further. Letâ€™s see this in action:\nAn important observation to make is that initially the green dot moves quickly and slows down as it gradually approaches the minima. The large slope (gradient) during the first few epochs (when the green dot is far from the minima) is responsible for this large update to the parameters. The gradient decreases as the green dot approaches the minima and thus the update becomes slow. The other three parameters are trained in parallel in the exact same way. Another important observation is that the shape of the curve changes with epoch. This is due to the fact that the other three parameters (w2, w3, b) are also being updated in parallel and each parameter contributes to the shape of the loss curve.\nVisualize Letâ€™s see how the decision boundary updates in real-time as the parameters are being updated.\nThatâ€™s all folks! If you made it till here, hats off to you! In this article, we took a visual approach to understand how machine learning works. So far, we have seen how a simple 3D to 1D mapping,Â f(x), can be used to fit a decision boundary (2D plane) to a linearly separable dataset (3D). We discussed how forward propagation is used to calculate the loss followed by backpropagation where gradients of the loss with respect to parameters are calculated and the parameters are updated repeatedly in a training loop.\n","permalink":"http://localhost:1313/blog/machine-learning-visualized/","summary":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write theÂ f(x)Â such that when users feed the inputÂ xÂ intoÂ f(x), it gives the correct outputÂ y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x).","title":"Machine Learning - Visualized"},{"content":"","permalink":"http://localhost:1313/arts/ranbirkapoor/","summary":"","title":"Ranbir Kapoor"},{"content":"\rDescription Our Fake News Detection Project leverages the power of machine learning to combat the proliferation of misleading information in the digital age. Designed for general users, journalists, and researchers, our platform allows easy verification of news articles, providing credibility assessments and offering educational resources to promote media literacy. Join us in the fight against fake news and the preservation of trustworthy information sources.\nğŸ”— GitHub ","permalink":"http://localhost:1313/projects/fake-news/","summary":"\rDescription Our Fake News Detection Project leverages the power of machine learning to combat the proliferation of misleading information in the digital age. Designed for general users, journalists, and researchers, our platform allows easy verification of news articles, providing credibility assessments and offering educational resources to promote media literacy. Join us in the fight against fake news and the preservation of trustworthy information sources.\nğŸ”— GitHub ","title":"Fake-News-Detection"},{"content":"ğŸ”— View App ğŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"ğŸ”— View App ğŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.","title":"Todo List App"},{"content":"Description\nCompleted the Developer and Technology Job Simulation where I developed an end-to-end understanding of the Software Development Lifecycle. Conducted in-depth research into emerging technology trends, particularly in the field of DevOps, demonstrating a proactive approach to staying informed about the rapidly evolving tech landscape. Created and delivered a compelling PowerPoint presentation analyzing and comparing Waterfall and Agile methodologies. Designed a custom algorithm using pseudocode and effectively communicated its logic by creating a detailed flow diagram. Debugged a program written in Python by fixing syntax and logic errors in the code. ","permalink":"http://localhost:1313/experience/accenture/","summary":"Description\nCompleted the Developer and Technology Job Simulation where I developed an end-to-end understanding of the Software Development Lifecycle. Conducted in-depth research into emerging technology trends, particularly in the field of DevOps, demonstrating a proactive approach to staying informed about the rapidly evolving tech landscape. Created and delivered a compelling PowerPoint presentation analyzing and comparing Waterfall and Agile methodologies. Designed a custom algorithm using pseudocode and effectively communicated its logic by creating a detailed flow diagram.","title":"Developer and Technology Intern"},{"content":"\r","permalink":"http://localhost:1313/experience/broadridge/","summary":"\r","title":"Software Engineer"},{"content":"ğŸ”— Sreedattha Description Happy Graduate âœ¨\n\u0026ldquo;When can you get happiness and sorrow at the same time?\u0026rdquo;\n\u0026ldquo;On the last day of exam,\u0026rdquo; The last year college student answered quietly :)\n","permalink":"http://localhost:1313/experience/jntuh/","summary":"ğŸ”— Sreedattha Description Happy Graduate âœ¨\n\u0026ldquo;When can you get happiness and sorrow at the same time?\u0026rdquo;\n\u0026ldquo;On the last day of exam,\u0026rdquo; The last year college student answered quietly :)","title":"Bachelor of Technology - B.Tech, Computer Science"}]